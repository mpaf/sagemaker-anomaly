{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop user_id, lock_id from all pandas dataframes\n",
    "\n",
    "X_train = X_train.drop(['user_id', 'lock_id', 'access_granted', 'is_weekend', 'is_business_hours', 'access_level', 'failed_attempts', 'time_of_day'], axis=1)\n",
    "X_test = X_test.drop(['user_id', 'lock_id', 'access_granted', 'is_weekend', 'is_business_hours', 'access_level', 'failed_attempts', 'time_of_day'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env MLFLOW_TRACKING_URI={mlflow_arn}\n",
    "%set_env MLFLOW_EXPERIMENT_NAME=anomaly_detection\n",
    "%set_env MLFLOW_RUN_NAME=training-kmeans\n",
    "%mkdir -p ./steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ./steps/training_kmeans.py\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "\n",
    "#@remote(instance_type=\"ml.m5.xlarge\")\n",
    "def train_kmeans(X_train, n_clusters=5, random_state=42, run_id=None):\n",
    "    import mlflow\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "    \n",
    "    if not run_id:\n",
    "        current_experiment=dict(mlflow.get_experiment_by_name(os.environ['MLFLOW_EXPERIMENT_NAME']))\n",
    "        experiment_id=current_experiment['experiment_id']   \n",
    "        run = MlflowClient().create_run(experiment_id=experiment_id, run_name=os.environ['MLFLOW_RUN_NAME'])\n",
    "        run_id = run.info.run_id\n",
    "        \n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        with mlflow.start_run(run_name='training', nested=True):\n",
    "            # Initialize and train KMeans    \n",
    "            # Enable autologging\n",
    "            mlflow.sklearn.autolog()\n",
    "\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=n_clusters,\n",
    "                init='k-means++',\n",
    "                n_init=10,\n",
    "                max_iter=300,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            \n",
    "            # Fit the model\n",
    "            kmeans.fit(X_train)\n",
    "            \n",
    "            # Get cluster assignments\n",
    "            cluster_labels = kmeans.predict(X_train)\n",
    "            \n",
    "            # Calculate clustering metrics\n",
    "            silhouette = silhouette_score(X_train, cluster_labels)\n",
    "            calinski = calinski_harabasz_score(X_train, cluster_labels)\n",
    "            inertia = kmeans.inertia_\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metrics({\n",
    "                \"silhouette_score\": silhouette,\n",
    "                \"calinski_harabasz_score\": calinski,\n",
    "                \"inertia\": inertia\n",
    "            })\n",
    "            \n",
    "            # Log cluster distribution\n",
    "            cluster_sizes = np.bincount(cluster_labels)\n",
    "            for i, size in enumerate(cluster_sizes):\n",
    "                mlflow.log_metric(f\"cluster_{i}_size\", int(size))\n",
    "                mlflow.log_metric(f\"cluster_{i}_percentage\", size / len(X_train) * 100)\n",
    "            \n",
    "            # Calculate and log cluster centers\n",
    "            for i, center in enumerate(kmeans.cluster_centers_):\n",
    "                mlflow.log_metrics({\n",
    "                    f\"cluster_{i}_center_{j}\": val \n",
    "                    for j, val in enumerate(center)\n",
    "                })\n",
    "            \n",
    "            return kmeans, cluster_labels\n",
    "\n",
    "def find_optimal_k(X_train, k_range=range(2, 11)):\n",
    "    \"\"\"Find optimal number of clusters using elbow method and silhouette score\"\"\"\n",
    "    import mlflow\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"kmeans_optimization\"):\n",
    "        inertias = []\n",
    "        silhouette_scores = []\n",
    "        \n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            kmeans.fit(X_train)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            inertia = kmeans.inertia_\n",
    "            silhouette = silhouette_score(X_train, kmeans.labels_)\n",
    "            \n",
    "            inertias.append(inertia)\n",
    "            silhouette_scores.append(silhouette)\n",
    "            \n",
    "            # Log metrics for each k\n",
    "            mlflow.log_metrics({\n",
    "                f\"k_{k}_inertia\": inertia,\n",
    "                f\"k_{k}_silhouette\": silhouette\n",
    "            })\n",
    "        \n",
    "        # Find optimal k using elbow method\n",
    "        # Calculate the differences between consecutive inertias\n",
    "        inertia_diffs = np.diff(inertias)\n",
    "        # Calculate the differences of differences to find the elbow point\n",
    "        inertia_diffs2 = np.diff(inertia_diffs)\n",
    "        # The elbow point is where the second derivative is maximum\n",
    "        optimal_k = k_range[np.argmax(inertia_diffs2) + 1]\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.plot(k_range, inertias, 'bo-')\n",
    "        plt.xlabel('Number of Clusters (k)')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.title('Elbow Method for Optimal k')\n",
    "        plt.grid(True)\n",
    "\n",
    "        mlflow.log_figure(fig, \"cluster_inertia.png\")\n",
    "        mlflow.log_param(\"optimal_k\", optimal_k)\n",
    "        \n",
    "        return optimal_k, inertias, silhouette_scores\n",
    "\n",
    "def analyze_clusters(X_train, cluster_labels, feature_names):\n",
    "    \"\"\"Analyze cluster characteristics\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"analyze_clusters\"):\n",
    "\n",
    "        # Create DataFrame with features and cluster labels\n",
    "        df_clustered = pd.DataFrame(X_train, columns=feature_names)\n",
    "        df_clustered['Cluster'] = cluster_labels\n",
    "        \n",
    "        # Calculate cluster profiles\n",
    "        cluster_profiles = df_clustered.groupby('Cluster').mean()\n",
    "        \n",
    "        # Log cluster profiles\n",
    "        for cluster in cluster_profiles.index:\n",
    "            for feature in feature_names:\n",
    "                mlflow.log_metric(\n",
    "                    f\"cluster_{cluster}_{feature}_mean\", \n",
    "                    cluster_profiles.loc[cluster, feature]\n",
    "                )\n",
    "        \n",
    "        return cluster_profiles\n",
    "\n",
    "# Main execution function\n",
    "def run_clustering_analysis(X_train, feature_names):\n",
    "    # Find optimal number of clusters\n",
    "    optimal_k, inertias, silhouette_scores = find_optimal_k(X_train)\n",
    "\n",
    "    # Train final model with optimal k\n",
    "    kmeans_model, cluster_labels = train_kmeans(X_train, n_clusters=optimal_k)\n",
    "    \n",
    "    # Analyze clusters\n",
    "    cluster_profiles = analyze_clusters(X_train, cluster_labels, feature_names)\n",
    "    \n",
    "    return kmeans_model, cluster_labels, cluster_profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from X_train\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Run clustering analysis\n",
    "model, labels, profiles = run_clustering_analysis(X_train, feature_names)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nCluster Profiles:\")\n",
    "print(profiles)\n",
    "\n",
    "print(\"\\nCluster Sizes:\")\n",
    "print(pd.Series(labels).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters_2d(X, labels, method='pca', title=None, save_path=None, centroids=None, sample_ids=None):\n",
    "    \"\"\"\n",
    "    Visualize clusters in 2D using either PCA or t-SNE\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Features matrix\n",
    "    labels : array-like\n",
    "        Cluster labels\n",
    "    method : str, default='pca'\n",
    "        Dimensionality reduction method ('pca' or 'tsne')\n",
    "    title : str, optional\n",
    "        Plot title\n",
    "    save_path : str, optional\n",
    "        Path to save the plot\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Reduce dimensionality\n",
    "    if method.lower() == 'pca':\n",
    "        reducer = PCA(n_components=2)\n",
    "        X_2d = reducer.fit_transform(X)\n",
    "        method_name = 'PCA'\n",
    "    else:\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "        X_2d = reducer.fit_transform(X)\n",
    "        method_name = 't-SNE'\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    df_plot = pd.DataFrame({\n",
    "        'x': X_2d[:, 0],\n",
    "        'y': X_2d[:, 1],\n",
    "        'Cluster': labels\n",
    "    })\n",
    "        \n",
    "    # Set up the plot style\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    fig, (ax1) = plt.subplots(1, 1)    \n",
    "    # Create scatter plot\n",
    "    sns.scatterplot(data=df_plot, x='x', y='y', hue='Cluster', \n",
    "                    palette='deep', s=50, alpha=0.7)\n",
    "\n",
    "    # Add centroids if provided\n",
    "    if centroids is not None and method.lower() == 'pca':\n",
    "        centroids_2d = reducer.transform(centroids)\n",
    "        ax1.scatter(centroids_2d[:, 0], centroids_2d[:, 1], \n",
    "                   c='black', marker='x', s=100, linewidths=3, \n",
    "                   label='Centroids')\n",
    "\n",
    "    # Annotate points if requested\n",
    "    if sample_ids is not None:\n",
    "        ax1.scatter(X_2d[sample_ids, 0], X_2d[sample_ids, 1], \n",
    "                c='red', marker='x', s=15, linewidths=1, \n",
    "                label='anomalies')\n",
    "           \n",
    "    # Add titles and labels\n",
    "    plt.title(title or f'Cluster Visualization using {method_name}')\n",
    "    plt.xlabel(f'{method_name} Component 1')\n",
    "    plt.ylabel(f'{method_name} Component 2')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pred = model.predict(X_test)\n",
    "X_test_dist = model.transform(X_test)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "X_test_anomaly_indexes = y_test.loc[y_test==1].index.tolist()\n",
    "\n",
    "dist_to_centroid = [dist_list[ind] for ind, dist_list in zip(X_test_pred, X_test_dist)]\n",
    "threshold = np.percentile(dist_to_centroid, 96)\n",
    "print(f\"Threshold: {threshold}\")\n",
    "# get max_values index above threshold\n",
    "anomaly_idx = np.where(dist_to_centroid > threshold)[0]\n",
    "print(f\"Detected anomalies {len(anomaly_idx)}\")\n",
    "print(f\"Groundtruth anomalies {len(X_test_anomaly_indexes)}\")\n",
    "\n",
    "# count number of matching ids between anomaly_idx and X_train_anomaly_indexes\n",
    "print(f\"Anomalies detected matching groundtruth {len(np.intersect1d(anomaly_idx, X_test_anomaly_indexes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters_2d(X_test, X_test_pred, title='Visualisation', centroids=profiles) #, sample_ids=X_test_anomaly_indexes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
