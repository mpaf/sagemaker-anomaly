{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment with SageMaker SKLearn Container\n",
    "\n",
    "This notebook demonstrates how to train a model locally using our custom training function and deploy it using the SageMaker SKLearn container. This approach allows us to test our training code locally before moving to cloud deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "\n",
    "%set_env MLFLOW_TRACKING_URI={mlflow_arn}\n",
    "%set_env MLFLOW_EXPERIMENT_NAME=anomaly_detection\n",
    "%set_env MLFLOW_RUN_NAME=training-kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ModelBuilder from sagemaker\n",
    "from steps.training_kmeans import train_kmeans\n",
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.spec.inference_spec import InferenceSpec\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from sagemaker.serve.mode.function_pointers import Mode\n",
    "import sagemaker\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "model, labels = train_kmeans(X_train, 3)\n",
    "\n",
    "#pickle model to disk\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pkl.dump(model, f)\n",
    "\n",
    "#tar.gz model file\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('model.pkl')\n",
    "\n",
    "# upload model to s3\n",
    "sagemaker_session = sagemaker.Session()\n",
    "model_s3_path = sagemaker_session.upload_data(path='model.tar.gz', bucket=sagemaker_session.default_bucket(), key_prefix='models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development and Testing with SageMaker\n",
    "\n",
    "SageMaker local mode allows you to develop and test your machine learning workflows on your local machine before deploying to the cloud. This helps accelerate the development cycle and reduce costs during the experimentation phase.\n",
    "\n",
    "## Key Benefits\n",
    "- Faster development iterations\n",
    "- Cost-effective testing\n",
    "- Early bug detection\n",
    "- No need for cloud resources during development\n",
    "- Same APIs as cloud deployment\n",
    "\n",
    "## Requirements\n",
    "- Docker installed locally\n",
    "- SageMaker Python SDK\n",
    "- Sufficient local compute resources\n",
    "- AWS credentials configured\n",
    "\n",
    "## Local Mode Components\n",
    "\n",
    "### Local Training\n",
    "```python\n",
    "# Example of local training\n",
    "estimator = Estimator(\n",
    "    ...,\n",
    "    instance_type='local'  # Use local mode\n",
    ")\n",
    "estimator.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_s3_path,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    source_dir='./src/',\n",
    "    py_version='py3',\n",
    "    framework_version='1.2-1',\n",
    "    entry_point='inference.py'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor = sklearn_model.deploy(\n",
    "    instance_type='local',\n",
    "    initial_instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, dists = predictor.predict(X_test[0:1000].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(dists > 16))\n",
    "np.where(y_test[0:1000].values == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Models to SageMaker Serverless Endpoints\n",
    "\n",
    "Serverless endpoints in SageMaker provide a way to deploy machine learning models without managing the underlying infrastructure. AWS automatically handles the compute resources, scaling, and infrastructure management.\n",
    "\n",
    "## Key Benefits\n",
    "- No infrastructure management required\n",
    "- Pay only for compute time used during inference\n",
    "- Automatic scaling based on workload\n",
    "- Reduced operational complexity\n",
    "\n",
    "## Deployment Process\n",
    "\n",
    "1. **Create a Model**\n",
    "   - Register your trained model in SageMaker\n",
    "   - Specify the model artifacts and inference code\n",
    "\n",
    "2. **Create Endpoint Configuration**\n",
    "   - Specify serverless configuration parameters:\n",
    "     - Memory size (1024 to 6144 MB)\n",
    "     - Maximum concurrency (1 to 200)\n",
    "   - Choose model variant(s) for deployment\n",
    "\n",
    "3. **Create Endpoint**\n",
    "   - Deploy using the endpoint configuration\n",
    "   - Wait for endpoint status to become \"InService\"\n",
    "\n",
    "## Example Configuration Parameters\n",
    "```python\n",
    "serverless_config = {\n",
    "    \"MemorySizeInMB\": 4096,\n",
    "    \"MaxConcurrency\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig\n",
    "\n",
    "sklearn_model_serverless = SKLearnModel(\n",
    "    model_data=model_s3_path,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    source_dir='./src/',\n",
    "    py_version='py3',\n",
    "    framework_version='1.2-1',\n",
    "    entry_point='inference.py'\n",
    ")\n",
    "\n",
    "predictor_serverless = sklearn_model_serverless.deploy(\n",
    "    serverless_inference_config=ServerlessInferenceConfig(\n",
    "        memory_size_in_mb=2048, max_concurrency=1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_serverless.predict(X_test[0:1].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
