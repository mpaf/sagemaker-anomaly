{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/synthetic_keylock_data.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nNumber of anomalies:\", sum(df['is_anomaly']))\n",
    "print(\"Number of normal events:\", sum(~df['is_anomaly']))\n",
    "print(\"\\nFeature columns:\", df.columns.tolist())\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env MLFLOW_TRACKING_URI=arn:aws:sagemaker:eu-central-1:559317267498:mlflow-tracking-server/mlflow-experiments\n",
    "%set_env MLFLOW_EXPERIMENT_NAME=anomaly-detection\n",
    "%set_env AWS_PROFILE=AWSAdministratorAccess-559317267498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import logging\n",
    "\n",
    "# set logging level to DEBUG\n",
    "logger = logging.getLogger('mlflow')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Create copy to avoid modifying original data\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import FunctionTransformer\n",
    "    import mlflow\n",
    "\n",
    "    # set mlflow experiment and server\n",
    "    mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])\n",
    "    mlflow.set_experiment(os.environ['MLFLOW_EXPERIMENT_NAME'])\n",
    "\n",
    "    with mlflow.start_run(run_name=\"data_preprocessing\") as run:\n",
    "\n",
    "        mlflow.autolog()\n",
    "        mlflow.log_param(\"input_rows\", df.shape[0])\n",
    "        # 1. Convert timestamp to datetime\n",
    "        df_processed['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "        # Feature Engineering\n",
    "        # Time-based features\n",
    "        df_processed['month'] = df_processed['timestamp'].dt.month\n",
    "        df_processed['day'] = df_processed['timestamp'].dt.day\n",
    "        df_processed['hour'] = df_processed['timestamp'].dt.hour\n",
    "        df_processed['minute'] = df_processed['timestamp'].dt.minute\n",
    "        \n",
    "        # Create time windows for access patterns\n",
    "        df_processed['time_of_day'] = pd.cut(df_processed['hour'], \n",
    "                                bins=[0, 6, 12, 18, 24], \n",
    "                                labels=['night', 'morning', 'afternoon', 'evening'])\n",
    "        \n",
    "        # Calculate access frequency features\n",
    "        df_processed['user_access_frequency'] = df.groupby('user_id')['timestamp'].transform('count')\n",
    "        df_processed['lock_access_frequency'] = df.groupby('lock_id')['timestamp'].transform('count')\n",
    "        \n",
    "        # Calculate average access duration per user and lock\n",
    "        df_processed['user_avg_duration'] = df.groupby('user_id')['access_duration'].transform('mean')\n",
    "        df_processed['lock_avg_duration'] = df.groupby('lock_id')['access_duration'].transform('mean')\n",
    "        \n",
    "        # Calculate failed attempts ratio\n",
    "        df_processed['failed_attempts_ratio'] = df['failed_attempts'] / (df.groupby('user_id')['failed_attempts'].transform('sum') + 1)\n",
    "            \n",
    "        categorical_cols = ['user_id', 'lock_id', 'access_level', 'time_of_day']\n",
    "\n",
    "        numerical_cols = ['access_duration', 'failed_attempts', 'time_since_last_access',\n",
    "                        'user_access_frequency', 'lock_access_frequency',\n",
    "                        'user_avg_duration', 'lock_avg_duration', 'failed_attempts_ratio']\n",
    "\n",
    "        boolean_cols = ['access_granted', 'is_weekend', 'is_business_hours', 'is_anomaly']\n",
    "        \n",
    "        # Apply transformations\n",
    "        transformer = ColumnTransformer([\n",
    "                ('numeric', MinMaxScaler(), numerical_cols),\n",
    "                ('categorical', OrdinalEncoder(), categorical_cols),\n",
    "                ('boolean', FunctionTransformer(lambda x: x.astype(int)), boolean_cols)\n",
    "            ],\n",
    "            remainder='drop'\n",
    "        )\n",
    "\n",
    "        # Fit and transform the data\n",
    "        df_processed = pd.DataFrame(transformer.fit_transform(df_processed), columns = numerical_cols + categorical_cols + boolean_cols)\n",
    "\n",
    "        mlflow.log_input(mlflow.data.from_pandas(df_processed), context=\"processed\")\n",
    "\n",
    "        print(f\"Current MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "        # Print current experiment info\n",
    "        current_experiment = mlflow.get_experiment(run.info.experiment_id)\n",
    "        if current_experiment:\n",
    "            print(f\"Current experiment name: {current_experiment.name}\")\n",
    "            print(f\"Current experiment ID: {current_experiment.experiment_id}\")\n",
    "            print(f\"Current experiment artifact location: {current_experiment.artifact_location}\")\n",
    "\n",
    "        # 4. Split features and target\n",
    "        X = df_processed.drop(['is_anomaly'], axis=1)\n",
    "        y = df_processed['is_anomaly']\n",
    "        \n",
    "        # 5. Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Locally\n",
    "\n",
    "Let's first run our data processing script locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_data(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run using the @remote decorator\n",
    "\n",
    "Let's run the script and dependencies using SageMaker's @remote decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1566392784.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[105], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    from sagemaker.remote_function import remoterom sagemaker.rsemote import remote\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.remote_function import remote\n",
    "\n",
    "preprocess_data = remote(preprocess_data, instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
